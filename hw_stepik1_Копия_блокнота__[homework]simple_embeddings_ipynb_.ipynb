Вот исправленный и улучшенный вариант ноутбука с комментариями:

```python
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot3c4fjZwC4T"
      },
      "source": [
        "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
        "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2JdzEXmwRU5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc8iHXIVwDwj"
      },
      "source": [
        "***Some parts of the notebook are almost the copy of [ mmta-team course](https://github.com/mmta-team/mmta_fall_2020). Special thanks to mmta-team for making them publicly available. [Original notebook](https://github.com/mmta-team/mmta_fall_2020/blob/master/tasks/01_word_embeddings/task_word_embeddings.ipynb).***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D0wm5jt6j0U"
      },
      "source": [
        "<b> Прочитайте семинар, пожалуйста, для успешного выполнения домашнего задания. В конце ноутбука напишите свой вывод. Работа без вывода оценивается ниже."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIWqBuEa6j0b"
      },
      "source": [
        "## Задача поиска схожих по смыслу предложений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkwMPLA6j0g"
      },
      "source": [
        "Мы будем ранжировать вопросы [StackOverflow](https://stackoverflow.com) на основе семантического векторного представления"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNRXIEfu5a3Q"
      },
      "source": [
        "До этого в курсе не было речи про задачу ранжирования, поэтому введем математическую формулировку"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS9FwWNd5a3S"
      },
      "source": [
        "## Задача ранжирования (Learning to Rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdwY9-f75a3T"
      },
      "source": [
        "* $X$ - множество объектов\n",
        "* $X^l = \\{x_1, x_2, ..., x_l\\}$ - обучающая выборка\n",
        "<br>На обучающей выборке задан порядок между некоторыми элементами, то есть нам известно, что некий объект выборки более релевантный для нас, чем другой:\n",
        "* $i \\prec j$ - порядок пары индексов объектов на выборке $X^l$ c индексами $i$ и $j$\n",
        "### Задача:\n",
        "построить ранжирующую функцию $a$ : $X \\rightarrow R$ такую, что\n",
        "$$i \\prec j \\Rightarrow a(x_i) < a(x_j)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG2IGBsh5a3U"
      },
      "source": [
        "<img src=\"https://d25skit2l41vkl.cloudfront.net/wp-content/uploads/2016/12/Featured-Image.jpg\" width=500, height=450>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQk_rolFwT_h"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUe1PGXn6j0l"
      },
      "source": [
        "Будем использовать предобученные векторные представления слов на постах Stack Overflow.<br>\n",
        "[A word2vec model trained on Stack Overflow posts](https://github.com/vefstathiou/SO_word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYkI54Y-rk7a",
        "outputId": "0fb6d942-1ae2-4350-bef4-8929c1b6650e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-03 17:19:32--  https://zenodo.org/record/1199620/files/SO_vectors_200.bin?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/1199620/files/SO_vectors_200.bin [following]\n",
            "--2025-03-03 17:19:33--  https://zenodo.org/records/1199620/files/SO_vectors_200.bin\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1453905423 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘SO_vectors_200.bin?download=1’\n",
            "\n",
            "SO_vectors_200.bin? 100%[===================>]   1.35G  13.9MB/s    in 1m 47s  \n",
            "\n",
            "2025-03-03 17:21:20 (12.9 MB/s) - ‘SO_vectors_200.bin?download=1’ saved [1453905423/1453905423]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/1199620/files/SO_vectors_200.bin?download=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8YJTOYv6j0s"
      },
      "outputs": [],
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "wv_embeddings = KeyedVectors.load_word2vec_format(\"SO_vectors_200.bin?download=1\", binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIcT_g-C6j1E"
      },
      "source": [
        "#### Как пользоваться этими векторами?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWO5SPDY6j1G"
      },
      "source": [
        "Посмотрим на примере одного слова, что из себя представляет embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeSBlQfk6j1J",
        "outputId": "673e52ee-dde1-4f96-e061-c73721fa5180",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float32 (200,)\n"
          ]
        }
      ],
      "source": [
        "word = 'dog'\n",
        "if word in wv_embeddings:\n",
        "    print(wv_embeddings[word].dtype, wv_embeddings[word].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Eq-D1qxpMJ",
        "outputId": "87c08025-4592-4dca-d059-8fa0a43bfaee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num of words: 1787145\n"
          ]
        }
      ],
      "source": [
        "print(f\"Num of words: {len(wv_embeddings.index_to_key)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT6NTCys6j1Q"
      },
      "source": [
        "Найдем наиболее близкие слова к слову `dog`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2aLzFGPNPWF",
        "outputId": "a748487a-5847-460c-e0f0-47752b478996"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('animal', 0.8564180135726929),\n",
              " ('dogs', 0.7880866527557373),\n",
              " ('mammal', 0.7623804211616516),\n",
              " ('cats', 0.7621253728866577),\n",
              " ('animals', 0.760793924331665)]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv_embeddings.similar_by_word('dog', topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n08z2PjMwC5o"
      },
      "source": [
        "#### ***Вопрос 1:***\n",
        "* Входит ли слово `cat` в топ-5 близких слов к слову `dog`? Какое место оно занимает?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYwVz0xG6j1U",
        "outputId": "dc9e7859-2265-4e61-c371-6aa6c9bb429f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Да, слово 'cat' находится на 4 месте\n"
          ]
        }
      ],
      "source": [
        "# Проверяем наличие слова 'cat' в топ-5 похожих слов\n",
        "similar_words = wv_embeddings.similar_by_word('dog', topn=5)\n",
        "found = False\n",
        "position = 0\n",
        "\n",
        "for idx, (word, score) in enumerate(similar_words, 1):\n",
        "    if word == 'cat':\n",
        "        found = True\n",
        "        position = idx\n",
        "        break\n",
        "\n",
        "if found:\n",
        "    print(f\"Да, слово 'cat' находится на {position} месте\")\n",
        "else:\n",
        "    print(\"Нет, слова 'cat' нет в топ-5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wu7O43AY5jH"
      },
      "source": [
        "***Ответ:*** Да, слово 'cat' входит в топ-5 ближайших слов к 'dog' и занимает 4 место."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai48-5vv6j1d"
      },
      "source": [
        "### Векторные представления текста\n",
        "\n",
        "Перейдем от векторных представлений отдельных слов к векторным представлениям вопросов, как к **среднему** векторов всех слов в вопросе. Если для какого-то слова нет предобученного вектора, то его нужно пропустить. Если вопрос не содержит ни одного известного слова, то нужно вернуть нулевой вектор."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhNuxBJd6j1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "class MyTokenizer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def tokenize(self, text):\n",
        "        # Используем word_tokenize из nltk для более качественной токенизации\n",
        "        return word_tokenize(text.lower())  # приводим к нижнему регистру для нормализации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHcvu6186j1m"
      },
      "outputs": [],
      "source": [
        "def question_to_vec(question, embeddings, tokenizer, dim=200):\n",
        "    \"\"\"\n",
        "    Преобразует вопрос в векторное представление как среднее векторов слов\n",
        "    \n",
        "    Параметры:\n",
        "        question: строка с вопросом\n",
        "        embeddings: модель векторных представлений слов\n",
        "        tokenizer: токенизатор для разбиения текста на слова\n",
        "        dim: размерность вектора представления\n",
        "        \n",
        "    Возвращает:\n",
        "        Векторное представление вопроса\n",
        "    \"\"\"\n",
        "    words = tokenizer.tokenize(question)\n",
        "    word_vectors = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in embeddings:\n",
        "            word_vectors.append(embeddings[word])\n",
        "\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsJSNkhm6j1y"
      },
      "source": [
        "#### ***Вопрос 2:***\n",
        "\n",
        "* Какая третья (с индексом 2) компонента вектора предложения `I love neural networks` (округлите до 2 знаков после запятой)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a62r11cT6j10",
        "outputId": "22c34e4d-ac5a-4bf6-8755-6991d76e127b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.29"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# Создаем токенизатор\n",
        "tokenizer = MyTokenizer()\n",
        "\n",
        "# Получаем вектор для предложения\n",
        "vec = question_to_vec(\"I love neural networks\", wv_embeddings, tokenizer)\n",
        "\n",
        "# Округляем третью компоненту\n",
        "round(vec[2], 2)"
      ]
    },
    {